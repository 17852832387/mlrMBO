---
title: "Multifid"
author: "Jakob R"
date: "29. August 2014"
output: pdf_document
---

Benchmark erklärung weiter unten.

# Einfacher Aufruf
Pakete laden etc. Arbeitsverzeichnis ist das mlrMBO git Stammverzeichnis.
```{r}
library("devtools")
library("BBmisc")
library("ParamHelpers")
load_all(".")
options(warn = 2)
```

Hier der normale Aufruf von multifid innerhalb von mbo. Es ist der code der Datei `multifid_call_design.R`

```{r, eval=FALSE}
task = load2("../2013-ml_big_data_tuning/datasets/waveform5000.RData")
task = makeClassifTask(id="Sonar", data=getTaskData(task), target="class")

lrn1 = makeLearner("classif.ksvm")
lrn1.par.set = makeParamSet(
  makeNumericParam("sigma", lower = -15, upper = 5, trafo = function(x) 2^x)
)
lrn2 = makeDownsampleWrapper(learner=lrn1, dw.stratify=TRUE) #FIXEM TRUE does not work?
lrn2.par.set = makeParamSet(
  makeNumericParam("dw.perc", lower=0, upper=1)
)
par.set = c(lrn1.par.set, lrn2.par.set)

makeObjFun = function(lrn, task, rsm = makeResampleDesc(method = "Holdout", split=2/3)) {
  force(lrn)
  force(task)
  force(rsm)
  #we could also implement a fixed holdout for each level here if provided par.set
  function(x) {
    # produce train test split manually. then downsample training further.
    lrn.local = setHyperPars(lrn, par.vals=x)
    y = resample(lrn.local, task, rsm, show.info=FALSE)$aggr[[1L]]
    return(y)
  }
}
objfun = makeObjFun(lrn2, task)
control = makeMBOControl(
  init.design.points = 9L, #distributed over the different levels, seems not to work for <5 each
  init.design.fun = maximinLHS,
  iters = 10,
  on.learner.error = "stop",
  show.learner.output = FALSE,
)
control = setMBOControlInfill(control = control, 
                              crit = "multiFid", 
                              opt = "focussearch", 
                              opt.restarts = 1L, 
                              opt.focussearch.maxit = 1L, 
                              opt.focussearch.points = 300L)
control = setMBOControlMultiFid(control = control, 
                                param = "dw.perc", 
                                lvls = c(0.1, 0.3, 1))
surrogat.model = makeLearner("regr.km", predict.type="se", nugget.estim = TRUE, jitter = TRUE)
result = mbo(fun = objfun, par.set = par.set, learner = surrogat.model, control = control, show.info = TRUE)
```
So könnte man es auswerten:
```{r, eval=FALSE}
pdf("multifid_steps.pdf", width=6, height=6)
for (i in seq_along(result$plot.data)) {
  plot = genGgplot(result$plot.data[[i]], subset.variable=c("response", "crit"), title = sprintf("Step %i", i))
  print(plot)
}
dev.off()
as.data.frame(result$opt.path)
result$y.hat
```

# Benchmark
Das Benchmark findet sich in den Dateien
* `multifid_compare.R` Diese Datei Sourcen
* `multifid_benchmark.R` Wrapper für openML und mlr benchmarks
* `multifid_benchmark_generic.R` Der Benchmark Code mit Plots und etc.

Der schnellste Weg zu einem Ergebnis:
```{r, eval=FALSE}
source("todo-files/multifid/multifid_benchmark.R")
source("todo-files/test_functions.R")

e.seed = 44137

# Define learner and parameter
e.lrn = makeLearner("classif.LiblineaRBinary", type = 1)
e.par.set = makeParamSet(
  makeNumericParam("cost", lower = -15, upper = 5, trafo = function(x) 2^x)
)
e.lvl = c(0.1, 0.3, 0.5, 0.7, 1)

openML.ids = c(spambase = 273)
openML.res = lapply(openML.ids, openMLBenchmark, e.seed = e.seed, e.lrn = e.lrn, e.par.set = e.par.set, e.lvl = e.lvl, alpha2fix = TRUE)

extractSubList(openML.res, "bench.table", simplify = FALSE)
```
