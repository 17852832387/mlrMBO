---
title: "Parallelization"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
vignette: >
  %\VignetteIndexEntry{Parallelization}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE, cache = FALSE}
library(mlrMBO)
set.seed(123)
knitr::opts_chunk$set(cache = TRUE, collapse = FALSE, results = 'hold')
knitr::knit_hooks$set(document = function(x){
  gsub("```\n*```r*\n*", "", x)
})
```

## Purpose

This vignette will give you a short introduction how **mlrMBO** can be configured to make use of multicore or other parallel infrastructures by evaluating multiple points (each on one CPU) per MBO iteration, to improve the speed of the model-based optimization.

## Parallelization through multi point proposals

Like always let's start with the function we want to optimize:
```{r objective_function}
library(mlrMBO)
obj.fun = makeBraninFunction()
# visualize the function
autoplot(obj.fun, render.levels = TRUE, show.optimum = TRUE)
```

In our example we will have *two CPUs* that we want to utelize for the evaluation of the black-box function.

```{r control}
library(mlrMBO)
ctrl = makeMBOControl(propose.points = 2)
```

There are multiple methods available to propse multiple points.
Each has it's advantages and disadvantages.
The choice is mostly bound to your choice of the infill criterion, so we will decide for that first.
As we have a real valued parameter space of our objective function, the **expected improvement** is usually the best choice

```{r infill}
ctrl = setMBOControlInfill(ctrl, crit = crit.ei)
```

The available methods to propose multiple points are as follows:
* `cl`: Proposes points by the constant liar strategy. 
This is usually a good choice for the **augmented expected - ** and the **expected improvement**.
The first point is optained by the ordinary infill optimization, like it is used to propose one point.
To obtain the 2nd point we assume that the evaluation at the first point is finished.
As we don't really have an outcome yet, we set the outcome to a lie.
Usually the minimum of all observed outcomes is chosen as the lie.
This *lie* is used to update the model in order to propose subsequent point. 
The procedure is repeated until `propose.points` proposals are generated.
* `cb`: As learnt from Vignette for *Mixed Space Optimization* we prefer the **lower/upper confidence bound** (CB) when we use a *random forest* as a regression method for the surrogate.
For the *CB* the parameter $\lambda$ is not fixed. 
This characteristic can be used to obtain multiple proposals by optimizing the *CB* for different lambda values.
The lambdas are drawn from an exp(1)-distribution.
* `multicrit`: Use a evolutionary multicriteria optimization. This is a (mu+1) type evolutionary algorithm and runs for `multicrit.maxit` generations. The population size is set to `propose.points`.


It is possible to parallelize the evaluation of the target function to speed up the computation. Internally the
evaluation of the target function is realized with the R package `parallelMap`. This package offers simple parallelization with various different backends. For details on the usage see the [parallelMap github page](https://github.com/berndbischl/parallelMap#parallelmap), which offers a nice tutorial and describes all possible backends thorougly.  For our usage we use a *multicore* backend. Note, that the multicore parallelization does _not_ work on windows machines.

```{r eval=FALSE}
library(mlrMBO)
library(parallelMap)

obj.fun = makeSphereFunction(1)
learner = makeLearner("regr.km", predict.type = "se", covtype = "matern3_2")
control = makeMBOControl()
control = setMBOControlTermination(control, iters = 3)
design = generateDesign(n = 6, par.set = getParamSet(obj.fun))

parallelStartMulticore(cpus = 2) # use 2 CPUs
res = mbo(obj.fun, learner = learner, control = control)
parallelStop()
```



Since mbo is a sequential method we cannot gain any improvement in the running time in this setup. But there are multiple two situations in which parallelization can be utilized  _noisy functions_ and _multi point proposals_.


## Noisy functions

If our target function is not deterministic (e.g. the accuracy of a machine learning model) it is advisable to take the crossvalidation error. For the crossvalidation error, the model is fitted multuple times on slightly different training data. This process can be easily parallelized with `parallelMap`.

```{r, eval = FALSE}
library(mlr)

fn = function(x) {
  lrn = makeLearner("classif.ksvm", par.vals = x)
  
  rdesc = makeResampleDesc("CV", iters = 10L)
  
  res = resample(learner = lrn, iris.task, rdesc, show.info = FALSE)
  
  res$aggr
}


par.set = filterParams(mlr::getParamSet("classif.ksvm"), ids = c("C", "sigma"))

par.set = makeParamSet(makeNumericParam("C", lower = -10, upper = 10, trafo = function(x) 2^x),
                       makeNumericParam("sigma", lower = -10, upper = 10, trafo = function(x) 2^x))

obj.fun = makeSingleObjectiveFunction(name = "noisy_example",
                                      fn = fn,
                                      has.simple.signature = FALSE,
                                      par.set = par.set)


control = makeMBOControl()
control = setMBOControlTermination(control, iters = 10L)

design = generateDesign(n = 10, getParamSet(obj.fun), trafo = TRUE)

parallelStartMulticore(cpus = 4L)
exampleRun(obj.fun, design = design, control = control)
parallelStop()

```

