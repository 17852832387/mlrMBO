---
title: "mlrMBO: Quick introduction"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
vignette: >
  %\VignetteIndexEntry{mlrMBO: Quick introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE, cache = FALSE}
library(mlrMBO)
set.seed(123)
knitr::opts_chunk$set(cache = TRUE, collapse = FALSE, results = 'hold')
knitr::knit_hooks$set(document = function(x){
  gsub("```\n*```r*\n*", "", x)
})
```

# mlrMBO: A toolbox for Model-Based Optimization of Expensive Black-Box Functions

This Vignette is supposed to give you a short introductory glance at the key features of `mlrMBO`.
More detailed and in depth tutorials can be found in the other vignette files on our GitHub project page:

- [Project Page](https://github.com/mlr-org/mlrMBO/)
- Online documentation of [mlrMBO](https://mlr-org.github.io/mlrMBO/)

## Purpose

The main goal of `mlrMBO` is to optimize *Expensive Black-Box Functions* through *Model-Based Optimization* and to provide a unified interface for different MBO flavours and optimization tasks, namely:

- Efficient global optimization of problems with numerical domain and Kriging as surrogate
- Multi-criteria optimization using different strategies
- Allowing arbitary regression models interfacing [mlr](https://github.com/mlr-org/mlr/) to be used as a surrogate
- Built-in parallelization using multi point proposals

## Quickstart

**This guide gives an overview of the typical optimization workflow with mlrMBO for those familiar with model based optimization.**

### Prerequisites

We demonstrate the main functionality of **mlrMBO** by optimizing different test functions.
Instead of writing these function by hand, we make use of the [smoof](https://cran.r-project.org/package=smoof) package which offers many single objective functions frequently used for benchmarking of optimizers.
[smoof](https://cran.r-project.org/package=smoof) is a dependency and get automatically attached with mlrMBO:
```{r load_package}
library(mlrMBO)
```
Note that you are not limited to these test functions bat can define arbitrary black box functions using the constructor `makeSingleObjectiveFunction()`.


### Example run: 1d numeric
For our simplest example we choose a cosine mixture function:
```{r cosine_fun}
obj.fun = makeCosineMixtureFunction(1)
obj.fun = convertToMinimization(obj.fun)
print(obj.fun)
ggplot2::autoplot(obj.fun, show.optimum = TRUE) # FIXME: optimum is very strange?
```
We decide to use Kriging as our surrogate model and to do 10 sequential optimization steps using the Expected Improvement (EI) as the infill criterion.
Furthermore, we provide an initial design on which we evaluate our first model in the beginning.
We use `ParamHelpers::generateDesign` to generate 10 points via a latin hypercube.
```{r cosine_setup}
learner = makeLearner("regr.km", predict.type = "se", covtype = "matern3_2",
  control = list(trace = FALSE))
control = makeMBOControl()
control = setMBOControlTermination(control, iters = 10)
control = setMBOControlInfill(control, crit = "ei")
design = generateDesign(n = 10, par.set = getParamSet(obj.fun))
```

Finally, we start the optimization process and print the result object.

```{r cosine_run}
run = mbo(obj.fun, design = design, learner = learner, control = control, show.info = TRUE)
print(run)
```


To get better insight into the MBO process, we can start the optimization with the function `exampleRun()` instead of `mbo()`.
This specialized function augments the results of `mbo()` with additional information for plotting.
Here, we opt to plot the optimization state at iterations 1, 3, and 10.
```{r cosine_examplerun, results="hide"}
run = exampleRun(obj.fun, learner = learner, control = control, show.info = FALSE)
```

```{r cosine_plot_examplerun, warning=FALSE}
print(run)
plotExampleRun(run, iters = c(1L, 3L, 10L), pause = FALSE)
```


### Example run: 2d numeric

We proceed with a slightly more realistic example and optimize the [Branin function](https://www.sfu.ca/~ssurjano/branin.html).
```{r branin_fun}
obj.fun2 = makeBraninFunction()
ggplot2::autoplot(obj.fun2, render.levels = TRUE, show.optimum = TRUE)
run2 = exampleRun(obj.fun2, learner = learner, control = control, show.info = FALSE)
print(run2)
plotExampleRun(run2, iters = c(1L, 3L, 10L))
```

